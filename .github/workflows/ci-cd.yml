name: CI/CD Pipeline

on:
  push:
    branches:
      - main

permissions:
  contents: write

jobs:
  manage-certificates:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      - name: Install OpenSSL
        run: |
          sudo apt-get update
          sudo apt-get install -y openssl
      - name: Manage SSL Certificates
        run: ./scripts/manage-certificates.sh
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
  version:
    needs: manage-certificates
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.bump.outputs.new_version }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history and tags!
      - name: Bump version
        id: bump
        run: |
          git fetch --tags
          latest=$(git tag --list 'v*' --sort=-v:refname | head -n1)
          if [ -z "$latest" ]; then
            latest="v0.0.0"
          fi
          echo "Latest tag: $latest"
          IFS='.' read -r major minor patch <<< "${latest#v}"
          if git log -1 --pretty=%B | grep -qE "fix:"; then
            patch=$((patch+1))
          elif git log -1 --pretty=%B | grep -qE "feat:"; then
            minor=$((minor+1)); patch=0
          else
            patch=$((patch+1))
          fi
          new_tag="v${major}.${minor}.${patch}"
          if git rev-parse "$new_tag" >/dev/null 2>&1; then
            echo "Tag $new_tag already exists. Skipping tag creation."
          else
            git tag $new_tag
            git push origin $new_tag
          fi
          echo "::set-output name=new_version::$new_tag"
  build-and-push:
    needs: version
    runs-on: ubuntu-latest
    outputs:
      should_build: ${{ steps.check_changes.outputs.should_build }}
      status: ${{ job.status }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for relevant changes
        id: check_changes
        run: |
          git diff --quiet HEAD^ HEAD -- Dockerfile "*.py" || CHANGED=true
          echo "Changes detected: ${CHANGED:-false}"
          echo "should_build=${CHANGED:-false}" >> "$GITHUB_OUTPUT"
          # Always exit successfully to not block other jobs
          exit 0

      - name: Set up QEMU
        if: steps.check_changes.outputs.should_build == 'true'
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        if: steps.check_changes.outputs.should_build == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        if: steps.check_changes.outputs.should_build == 'true'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push image
        if: steps.check_changes.outputs.should_build == 'true'
        uses: docker/build-push-action@v3
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/barkuni-api:${{ needs.version.outputs.version }}
            ${{ secrets.DOCKERHUB_USERNAME }}/barkuni-api:latest
          platforms: linux/amd64,linux/arm64
  deploy-infra:
    needs: [build-and-push, manage-certificates]
    # Run if build was successful OR was skipped due to no changes
    if: |
      always() &&
      (needs.build-and-push.result == 'success' ||
       (needs.build-and-push.result == 'skipped' && needs.build-and-push.outputs.should_build == 'false'))
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
      CLUSTER_NAME: barkuni-exam-cluster
    steps:
      - uses: actions/checkout@v4
      
      - name: Install AWS CLI
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install --update
          aws --version

      - name: Install Terraform
        run: |
          wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg
          echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
          sudo apt-get update
          sudo apt-get install -y terraform
          terraform version

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Verify Network Connectivity
        run: |
          # Get EKS endpoint
          ENDPOINT=$(aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.endpoint" --output text | sed 's|https://||')
          echo "EKS Endpoint: $ENDPOINT"
          
          # Test connectivity
          nc -zv $ENDPOINT 443 || echo "Direct connection failed"
          curl -k -v https://$ENDPOINT/healthz || echo "HTTPS connection failed"
          
          # Get VPC info
          aws ec2 describe-vpcs --vpc-id vpc-02fb1f16ffa2c1a11
          
          # Get security group info
          aws ec2 describe-security-groups --filters Name=vpc-id,Values=vpc-02fb1f16ffa2c1a11

      - name: Terraform Init and Apply (Infrastructure)
        run: |
          cd terraform/infrastructure
          terraform init -backend=false
          # Copy terraform.tfvars values as CLI arguments in case tfvars file is not picked up
          terraform apply -auto-approve \
            -var="aws_region=$AWS_DEFAULT_REGION" \
            -var="eks_cluster_name=$CLUSTER_NAME" \
            -var="vpc_id=vpc-02fb1f16ffa2c1a11" \
            -var="cloudwatch_log_group_name=/aws/eks/barkuni-exam-cluster/cluster" \
            -var="kms_key_id=arn:aws:kms:us-east-1:058264138725:key/c600ebf9-94ec-4cf6-9e5a-1403967190d2" \
            -var="domain_name=vicarius.xyz" \
            -var="organization_name=Barkuni" \
            -var="node_group_name=general" \
            -var="node_role_arn=arn:aws:iam::058264138725:role/general-eks-node-group-20250520133836818000000001" \
            -var="node_desired_size=2" \
            -var="node_min_size=1" \
            -var="node_max_size=3" \
            -var='node_instance_types=["t3.medium"]' \
            -var="project_name=barkuni"
      - name: Wait for EKS Cluster and Node Group
        run: |
          for i in {1..30}; do 
            STATUS=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_DEFAULT_REGION --query "cluster.status" --output text)
            echo "EKS cluster status: $STATUS"
            if [ "$STATUS" = "ACTIVE" ]; then break; fi
            sleep 20
          done
          for i in {1..30}; do 
            STATUS=$(aws eks describe-nodegroup --cluster-name $CLUSTER_NAME --nodegroup-name general --region $AWS_DEFAULT_REGION --query "nodegroup.status" --output text)
            echo "Node group status: $STATUS"
            if [ "$STATUS" = "ACTIVE" ]; then break; fi
            sleep 20
          done
      - name: Wait for EKS API Server
        run: |
          echo "Checking EKS API server connectivity..."
          for i in {1..45}; do
            if aws eks describe-cluster --name $CLUSTER_NAME --query cluster.status --output text; then
              if curl -k -m 10 "https://$(aws eks describe-cluster --name $CLUSTER_NAME --query 'cluster.endpoint' --output text | sed 's/https:\/\///')/healthz"; then
                echo "EKS API is accessible"
                break
              fi
            fi
            echo "Waiting for EKS API to be accessible... Attempt $i"
            sleep 20
          done

      - name: Configure kubectl with retry
        run: |
          # Get cluster endpoint
          CLUSTER_ENDPOINT=$(aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.endpoint" --output text)
          echo "Cluster endpoint: $CLUSTER_ENDPOINT"
          
          # Configure AWS CLI to use v1 for eks commands
          aws configure set default.eks.use-v1-api-endpoints true
          
          # Configure kubectl without network config option
          aws eks update-kubeconfig \
            --name $CLUSTER_NAME \
            --region $AWS_DEFAULT_REGION
          
          # Wait for API server with increased timeout
          for i in {1..30}; do
            if kubectl get nodes --request-timeout=30s; then
              echo "Successfully connected to EKS cluster"
              break
            fi
            echo "Retrying connection to EKS API... Attempt $i"
            sleep 10
          done

      - name: Install Prerequisites and Helm
        run: |
          # Install OpenSSL and other required packages
          apt-get update && apt-get install -y curl openssl ca-certificates

          # Install Helm with verification disabled (since we're in a container)
          export VERIFY_CHECKSUM=false
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh
          helm version

      - name: Install AWS Load Balancer Controller
        run: |
          # Add EKS chart repo
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          # Install the AWS Load Balancer Controller
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=$CLUSTER_NAME \
            --set serviceAccount.create=true \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=$AWS_DEFAULT_REGION \
            --set vpcId=vpc-02fb1f16ffa2c1a11 \
            --set image.repository=602401143452.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/amazon/aws-load-balancer-controller \
            --debug \
            --wait

          # Verify installation
          kubectl get deployment -n kube-system aws-load-balancer-controller
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller

      - name: Install NGINX Ingress
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          
          echo "Current cluster state:"
          kubectl get nodes -o wide
          kubectl get pods -A
          
          # Remove existing deployment if exists
          helm uninstall ingress-nginx -n ingress-nginx || true
          kubectl delete namespace ingress-nginx --timeout=5m || true
          sleep 30  # Increased sleep to ensure cleanup
          
          # Debug cluster state after cleanup
          echo "Cluster state after cleanup:"
          kubectl get ns
          kubectl get pods -A
          
          # Install with debug flags and increased timeout
          HELM_DEBUG=true helm install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --debug \
            --dry-run > ingress-nginx-debug.yaml
          
          echo "Dry run output:"
          cat ingress-nginx-debug.yaml
          
          HELM_DEBUG=true helm install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --debug \
            --set controller.kind=Deployment \
            --set controller.replicaCount=1 \
            --set controller.minReadySeconds=10 \
            --set-string controller.progressDeadlineSeconds=600 \
            --set controller.service.enabled=true \
            --set controller.service.type=LoadBalancer \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="nlb" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-scheme"="internet-facing" \
            --set controller.publishService.enabled=true \
            --set controller.config.timeout-connect=120s \
            --set controller.config.timeout-read=120s \
            --set controller.config.timeout-write=120s \
            --timeout 20m \
            --wait
          
          echo "Post-installation state:"
          kubectl get all -n ingress-nginx
          kubectl describe deployment ingress-nginx-controller -n ingress-nginx
          kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller --tail=100
      - name: Get NGINX Ingress LB DNS Name
        id: nginx_lb
        run: |
          for i in {1..30}; do
            LB_DNS=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
            if [ -n "$LB_DNS" ]; then
              echo "nginx_lb_dns_name=$LB_DNS" >> $GITHUB_OUTPUT
              break
            fi
            echo "Waiting for NGINX Ingress LB DNS name..."
            sleep 10
          done
          if [ -z "$LB_DNS" ]; then
            echo "ERROR: NGINX Ingress LoadBalancer DNS name not found." >&2
            exit 1
          fi
  deploy-dns:
    needs: deploy-infra
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Terraform
        run: |
          wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg
          echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
          sudo apt-get update
          sudo apt-get install -y terraform
          terraform version
          echo "nginx_lb_dns_name=${{ steps.nginx_lb.outputs.nginx_lb_dns_name }}"   
      - name: Terraform Init (DNS)    
        run: |       
          cd terraform/dns        
          terraform init -backend=false
      - name: Terraform Apply (DNS)
        run: |   
          cd terraform/dns            
          terraform apply -auto-approve -var="nginx_lb_dns_name=${{ steps.nginx_lb.outputs.nginx_lb_dns_name }}"